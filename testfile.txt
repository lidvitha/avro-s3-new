
private void flushRecords(String topic) {
        if (!topicBuffers.get(topic).isEmpty()) {
            try {
                String key = String.format("%s/%s", topic, topicFileKeys.getOrDefault(topic, generateFileKey()));
                Path tempFile = Files.createTempFile("parquet", ".parquet");

                // Write records to Parquet file
                try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new Path(tempFile.toString()))
                        .withSchema(NonDecomposeFormat.getClassSchema())
                        .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)
                        .withCompressionCodec(CompressionCodecName.SNAPPY)
                        .build()) {

                    for (SinkRecord record : topicBuffers.get(topic)) {
                        NonDecomposeFormat nonDecomposeFormat = createNonDecomposeFormatFromRecord(record);
                        writer.write(nonDecomposeFormat);
                    }
                }

                // Upload Parquet file to S3
                s3Client.putObject(new PutObjectRequest(bucketName, key, tempFile.toFile()));
                topicBuffers.get(topic).clear();
                topicLastFlushTimes.put(topic, System.currentTimeMillis());
                Files.delete(tempFile);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    private NonDecomposeFormat createNonDecomposeFormatFromRecord(SinkRecord record) {
        NonDecomposeFormat nonDecomposeFormat = new NonDecomposeFormat();
        nonDecomposeFormat.setEventId(String.valueOf(getHeaderValue(record, "eventId")));
        nonDecomposeFormat.setPartition(String.valueOf(record.kafkaPartition()));
        nonDecomposeFormat.setPayload(record.value().toString());
        nonDecomposeFormat.setUpdDate(String.valueOf(getHeaderValue(record, "updDate")));
        nonDecomposeFormat.setPublishTimestampStr(String.valueOf(getHeaderValue(record, "publishTimestampStr")));
        nonDecomposeFormat.setPayload1(String.valueOf(getHeaderValue(record, "payload1")));
        nonDecomposeFormat.setTopic(record.topic());
        nonDecomposeFormat.setEventName(String.valueOf(getHeaderValue(record, "eventName")));
        nonDecomposeFormat.setEventVersionNumber(String.valueOf(getHeaderValue(record, "eventVersionNumber")));
        nonDecomposeFormat.setExecutionTimestampStr(String.valueOf(getHeaderValue(record, "executionTimestampStr")));
        return nonDecomposeFormat;
    }

    private Object getHeaderValue(SinkRecord record, String headerKey) {
        Header header = record.headers().lastWithName(headerKey);
        return header != null ? header.value() : null;
    }
}






-----------------------------------
private NonDecomposeFormat createNonDecomposeFormatFromRecord(SinkRecord record) {
    NonDecomposeFormat nonDecomposeFormat = new NonDecomposeFormat();
    nonDecomposeFormat.setEventId(String.valueOf(getHeaderValue(record, "eventId")));
    nonDecomposeFormat.setPartition(String.valueOf(record.kafkaPartition()));
    nonDecomposeFormat.setPayload(record.value().toString());
    nonDecomposeFormat.setUpdDate(String.valueOf(getHeaderValue(record, "updDate")));
    nonDecomposeFormat.setPublishTimestampStr(String.valueOf(getHeaderValue(record, "publishTimestampStr")));
    nonDecomposeFormat.setPayload1(String.valueOf(getHeaderValue(record, "payload1")));
    nonDecomposeFormat.setTopic(record.topic());
    nonDecomposeFormat.setEventName(String.valueOf(getHeaderValue(record, "eventName")));
    nonDecomposeFormat.setEventVersionNumber(String.valueOf(getHeaderValue(record, "eventVersionNumber")));
    nonDecomposeFormat.setExecutionTimestampStr(String.valueOf(getHeaderValue(record, "executionTimestampStr")));
    return nonDecomposeFormat;
}

private Object getHeaderValue(SinkRecord record, String headerKey) {
    Header header = record.headers().lastWithName(headerKey);
    return header != null ? header.value() : null;
}







"Test content" 
copy custom-s3-sink-connector-0.0.1-SNAPSHOT.jar C:\kafka-connect-config\plugins
C:\kafka\bin\windows\connect-standalone.bat C:\kafka-connect-config\connect-standalone.properties C:\kafka-connect-config\s3-sink-connector.properties

bootstrap.servers=localhost:9092
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
offset.storage.file.filename=C:/kafka-connect-config/connect.offsets
plugin.path=C:/kafka-connect-config/plugins

name=s3-sink-connector
connector.class=com.example.S3SinkConnector
tasks.max=1
topics=new-test-topic
s3.bucket.name=demo-kafka-s3
s3.region=us-east-1
s3.batch.size=10
s3.batch.time.ms=60000
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter
